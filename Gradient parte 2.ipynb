{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c Derive the expressions for the derivatives of the regularized loss in Eq. (6) w.r.t. $W^{(1)}, b^{(1)}, b^{(2)}$ now.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start deriving the loss J w.r.t $W^{(1)}$, by applying the chain rule. We'll had then the regularization term. Applying the chain rule we get for $W^{(1)}$ the following:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial W^{(1)}} &= \\frac{\\partial J}{\\partial z^{(2)}} \\cdot \\frac{\\partial z^{(2)}}{\\partial W^{(1)}}\\\\\n",
    "\\end{align}\n",
    "\n",
    "For $\\frac{\\partial J}{\\partial z^{(2)}}$ we apply iteratively the chain rule obtaining:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial z^{(2)}} &= \\frac{\\partial J}{\\partial a^{(2)}} \\odot \\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\\\\n",
    "&= \\left( \\frac{\\partial J}{\\partial z^{(3)}} \\cdot \\frac{\\partial z^{(3)}}{\\partial a^{(2)}} \\right) \\odot \\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\\\\n",
    "\\end{align}\n",
    "\n",
    "Where $\\odot$ inidicates the element-wise product (Hadamard product) and $\\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\in \\mathbb{R}^{10 \\times 5}$ is the the derivative of the ReLu activation w.r.t. $z^{(2)}$, which is also the heaviside step function:\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a_{ij}^{(2)}}{\\partial z_{ij}^{(2)}} = \n",
    "\\left \\{\n",
    "\\begin{align}\n",
    "&1 \\quad if \\quad z^{(2)}_{ij} > 0\\\\\n",
    "&0 \\quad otherwise\n",
    "\\end{align}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "We do already know $\\frac{\\partial J}{\\partial z^{(3)}}$, now the calculation of the other partial derivatives is straightforward:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial z^{(3)}}{\\partial a^{(2)}} &= \\frac{\\partial \\left(W^{(2)} a^{(2)} + b^{(2)} \\right)}{\\partial a^{(2)}} = W^{(2)} \\\\\n",
    "\\frac{\\partial z^{(2)}}{\\partial W^{(1)}} &= \\frac{\\partial \\left(W^{(1)} a^{(1)} + b^{(1)} \\right)}{\\partial W^{(1)}} = a^{(1)}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we finally obtain the loss derivatives w.r.t. $W^{(1)}$ as it follows:\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial W^{(1)}} &= \\frac{\\partial J}{\\partial z^{(2)}} \\cdot \\frac{\\partial z^{(2)}}{\\partial W^{(1)}}\\\\ \n",
    "&= \\left[ \\left( \\frac{\\partial J}{\\partial z^{(3)}} \\cdot \\frac{\\partial z^{(3)}}{\\partial a^{(2)}} \\right) \\odot \\frac{\\partial a^{(2)}}{\\partial z^{(2)}}\\right] \\cdot \\frac{\\partial z^{(2)}}{\\partial W^{(1)}} \\\\\n",
    "&= \\frac{1}{N} \\cdot \\left\\{ \\left[ W^{(2)^T} \\cdot \\left(\\psi(z^{(3)}) - \\Delta \\right) \\right] \\odot \\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\right\\} \\cdot a^{(1)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now find the gradient of $W^{(1)}$ w.r.t the regularization term, which is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\left[ \\lambda \\left( \\left\\lVert W^{(1)}\\right\\rVert^2_2 + \\left\\lVert W^{(2)}\\right\\rVert^2_2 \\right) \\right]}{\\partial W^{(1)}} = 2 \\lambda W^{(1)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence we end up having:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\tilde{J}}{\\partial W^{(1)}} = \\frac{1}{N} \\cdot \\left\\{ \\left[ W^{(2)^T} \\cdot \\left(\\psi(z^{(3)}) - \\Delta \\right) \\right] \\odot \\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\right\\} \\cdot a^{(1)} +  2 \\lambda W^{(1)}\n",
    "$$\n",
    "\n",
    "Which can be also implemented in vectorized form using numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "\n",
    "We now derive the loss J w.r.t $b^{(1)}$. Applying the chain rule we obtain similarly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial b^{(1)}} &= \\frac{\\partial J}{\\partial z^{(2)}} \\cdot \\frac{\\partial z^{(2)}}{\\partial b^{(1)}}\\\\\n",
    "&= \\left[ \\left( \\frac{\\partial J}{\\partial z^{(3)}} \\cdot \\frac{\\partial z^{(3)}}{\\partial a^{(2)}} \\right) \\odot \\frac{\\partial a^{(2)}}{\\partial z^{(2)}}\\right] \\cdot \\frac{\\partial z^{(2)}}{\\partial W^{(1)}} \\\\\n",
    "&= \\left[ \\left( \\frac{\\partial J}{\\partial z^{(3)}} \\cdot \\frac{\\partial z^{(3)}}{\\partial a^{(2)}} \\right) \\odot \\frac{\\partial a^{(2)}}{\\partial z^{(2)}}\\right] \\cdot  \\mathbb{1} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search top results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('HW2/results_twolayernet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>num_iters</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>reg</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75</td>\n",
       "      <td>2000</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1.290952</td>\n",
       "      <td>1.401017</td>\n",
       "      <td>0.550286</td>\n",
       "      <td>0.508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>2000</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.100</td>\n",
       "      <td>1.301690</td>\n",
       "      <td>1.400054</td>\n",
       "      <td>0.548347</td>\n",
       "      <td>0.513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>75</td>\n",
       "      <td>2000</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.292343</td>\n",
       "      <td>1.426846</td>\n",
       "      <td>0.547327</td>\n",
       "      <td>0.490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>2000</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.316092</td>\n",
       "      <td>1.389855</td>\n",
       "      <td>0.541347</td>\n",
       "      <td>0.509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75</td>\n",
       "      <td>2000</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.250</td>\n",
       "      <td>1.331517</td>\n",
       "      <td>1.401600</td>\n",
       "      <td>0.539041</td>\n",
       "      <td>0.511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hidden_size  num_iters  batch_size  learning_rate    reg  train_loss  \\\n",
       "3           75       2000         200          0.001  0.010    1.290952   \n",
       "0           75       2000         200          0.001  0.100    1.301690   \n",
       "6           75       2000         200          0.001  0.001    1.292343   \n",
       "2           50       2000         200          0.001  0.001    1.316092   \n",
       "1           75       2000         200          0.001  0.250    1.331517   \n",
       "\n",
       "   val_loss  train_acc  val_acc  \n",
       "3  1.401017   0.550286    0.508  \n",
       "0  1.400054   0.548347    0.513  \n",
       "6  1.426846   0.547327    0.490  \n",
       "2  1.389855   0.541347    0.509  \n",
       "1  1.401600   0.539041    0.511  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorted by train accuracy\n",
    "df.sort_values(['train_acc'], ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>num_iters</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>reg</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>2000</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.100</td>\n",
       "      <td>1.301690</td>\n",
       "      <td>1.400054</td>\n",
       "      <td>0.548347</td>\n",
       "      <td>0.513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75</td>\n",
       "      <td>2000</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.250</td>\n",
       "      <td>1.331517</td>\n",
       "      <td>1.401600</td>\n",
       "      <td>0.539041</td>\n",
       "      <td>0.511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>2000</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.316092</td>\n",
       "      <td>1.389855</td>\n",
       "      <td>0.541347</td>\n",
       "      <td>0.509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75</td>\n",
       "      <td>2000</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1.290952</td>\n",
       "      <td>1.401017</td>\n",
       "      <td>0.550286</td>\n",
       "      <td>0.508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>2000</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.100</td>\n",
       "      <td>1.326783</td>\n",
       "      <td>1.446215</td>\n",
       "      <td>0.535673</td>\n",
       "      <td>0.499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hidden_size  num_iters  batch_size  learning_rate    reg  train_loss  \\\n",
       "0           75       2000         200          0.001  0.100    1.301690   \n",
       "1           75       2000         200          0.001  0.250    1.331517   \n",
       "2           50       2000         200          0.001  0.001    1.316092   \n",
       "3           75       2000         200          0.001  0.010    1.290952   \n",
       "4           50       2000         200          0.001  0.100    1.326783   \n",
       "\n",
       "   val_loss  train_acc  val_acc  \n",
       "0  1.400054   0.548347    0.513  \n",
       "1  1.401600   0.539041    0.511  \n",
       "2  1.389855   0.541347    0.509  \n",
       "3  1.401017   0.550286    0.508  \n",
       "4  1.446215   0.535673    0.499  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorted by validation accuracy\n",
    "df.sort_values(['val_acc'], ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
