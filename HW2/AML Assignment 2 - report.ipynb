{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> Fabio Montello (1834411), Francesco Russo (1449025), Michele Cernigliaro (1869097)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "In this report we proceed to answer as requested the questions 2 (a, b, c), 3 (a,b) and 4 (a,b,c). For each point we are going to write a brief description using figures and formulas whenever needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verify that the loss function defined in Eq. (5) has the gradient w.r.t. $z^{(3)}$ as below:**\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac { \\partial J } { \\partial z ^ { ( 3 ) } } ( \\{ x _ { i } , y _ { i } \\} _ { i = 1 } ^ { N } ) = \\frac { 1 } { N } ( \\psi ( z ^ { ( 3 ) } ) - \\Delta )\n",
    "\\end{equation*}\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\displaystyle{J = \\frac{1}{N} \\left[ \\sum_{i=1}^N -log \\space \\psi(z_{yi}) \\right] = \\frac{1}{N} \\left[ \\sum_{i=1}^N -log \\displaystyle{ \\frac{e^{z_{y_i}}}{\\sum_j e^{z_j}} } \\right] = \\frac{1}{N} \\left[ \\sum_{i=1, j = k_i}^N -log \\displaystyle{ \\frac{e^{z_{i,j}}}{\\sum_j e^{z_j}} } \\right]}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where we use the notation $z = z^{(3)}$ and the softmax activation function is \n",
    "\n",
    "<center>\n",
    "$\\displaystyle{ \\psi(z_{yi}) = \\frac{e^{z_{y_i}}}{\\sum_j e^{z_j}} = \\frac{e^{z_{i k_i}}}{\\sum_j e^{z_j}} }$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix $\\nabla_J$ of the derivatives of $J$ with respect to $z_{ij}$ for every $i = 1...N$ and for every $j = 1...K$ has the general element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\displaystyle{\n",
    "\\nabla_{J_{i',j'}} = \\frac{\\partial J}{\\partial z_{i'j'}} = \n",
    "\\frac{\\partial}{\\partial z_{i'j'}} \\frac{1}{N} \\left[ \\sum_{i=1}^N -log \\space \\psi(z_{yi}) \\right] = \n",
    "- \\frac{\\partial}{\\partial z_{i'j'}} \\frac{1}{N} \\left[log \\space \\psi(z_{yi'}) \\right] = \n",
    "- \\frac{1}{N} \\frac{1}{\\psi(z_{yi'})} \\frac{\\partial}{\\partial z_{i'j'}} \\psi(z_{yi'}) =\n",
    "}$$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\displaystyle{\n",
    "- \\frac{1}{N} \\frac{1}{\\psi(z_{yi'})} \\frac{e^{z_{i'j'}} \\cdot \\delta(j', k_{i'}) \\sum_j e^{z_{i'j}} - e^{z_{i'k_i'}}e^{z_{i'j'}}}{(\\sum_j e^{z_{i'j}})^2} = \n",
    "}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$$\\displaystyle{\n",
    "- \\frac{1}{N} \\frac{\\sum_j e^{z_{i'j}}}{e^{z_{i'k_i'}}} \\frac{e^{z_{i'j'}} \\cdot \\delta(j', k_{i'}) \\sum_j e^{z_{i'j}} - e^{z_{i'k_i'}}e^{z_{i'j'}}}{(\\sum_j e^{z_{i'j}})^2} =\n",
    "}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\displaystyle{\n",
    "\\frac{1}{N} \\left[\\frac{e^{z_{i'j'}}}{\\sum_j e^{z_{i'j}}} - \\delta(j', k_i') \\right]\n",
    "}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\delta(j', k_i')$ is the Kronecker Delta function\n",
    "\n",
    "In matrix form this is exactly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\displaystyle{\n",
    "\\nabla_J = \\frac{1}{N} \\left[\\psi(z) -  \\Delta_{j, ki} \\right]\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verify that the partial derivative of the loss w.r.t. $W^{(2)}$ is:**\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac { \\partial J } { \\partial W ^ { ( 2 ) } } ( \\{ x _ { i } , y _ { i } \\} _ { i = 1 } ^ { N } ) &= \\frac{\\partial J}{ \\partial z^{(3)}} \\cdot \\frac{\\partial z^{(3)}}{\\partial W^{(2)}} \\\\\n",
    "&= \\frac { 1 } { N } ( \\psi ( z ^ { ( 3 ) } ) - \\Delta ) a ^ { ( 2 ) }\n",
    "\\end{align*}\n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen before, we compute the chain rule as follow:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac { \\partial J } { \\partial W ^ { ( 2 ) } } ( \\{ x _ { i } , y _ { i } \\} _ { i = 1 } ^ { N } ) = \\frac{\\partial J}{ \\partial z^{(3)}} \\cdot \\frac{\\partial z^{(3)}}{\\partial W^{(2)}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already computed:\n",
    "$$ \\frac{\\partial J}{\\partial z^{(3)}} = \\frac { 1 } { N } ( \\psi ( z ^ { ( 3 ) } ) - \\Delta )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that $$\\frac{\\partial z^{(3)}}{\\partial W^{(2)}} = a ^ { ( 2 ) }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so we have that:\n",
    "$$\\frac { \\partial J } { \\partial W ^ { ( 2 ) } } ( \\{ x _ { i } , y _ { i } \\} _ { i = 1 } ^ { N } ) = \\frac { 1 } { N } ( \\psi ( z ^ { ( 3 ) } ) - \\Delta ) a ^ { ( 2 ) }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Similarly, verify that the regularized loss in Eq. (6) has the derivatives**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\tilde{J}}{\\partial W^{(2)}} = \\frac { 1 } { N } ( \\psi ( z ^ { ( 3 ) } ) - \\Delta ) a ^ { ( 2 ) } + 2 \\lambda W^{(2)}\n",
    "$$\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now find the partial derivative of the loss w.r.t the regularization term, which is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\left[ \\lambda \\left( \\left\\lVert W^{(1)}\\right\\rVert^2_2 + \\left\\lVert W^{(2)}\\right\\rVert^2_2 \\right) \\right]}{\\partial W^{(2)}} = 2 \\lambda W^{(2)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we obtain:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\displaystyle{\\frac{d\\tilde{J}}{dW^{(2)}} = \\frac{dJ}{dz^{(3)}} \\cdot \\frac{dz^{(3)}}{dW^{(2)}} = \\frac{1}{N}\\left[\\psi(z^{(3)}) - \\Delta \\right] \\cdot a^{(2)} + 2 \\lambda W^{(2)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2c\n",
    "\n",
    "**We can repeatedly apply chain rule as discussed above to obtain the derivatives of the loss with respect to all the parameters of the model $\\theta =\\left(W^{(1)}, b^{(1)}, W^{(2)}, b^{(2)}\\right)$. Derive the expressions for the derivatives of the regularized loss in Eq. (6) w.r.t. $W^{(1)}, b^{(1)}, b^{(2)}$ now.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start deriving the loss J w.r.t $W^{(1)}$, by applying the chain rule. We'll had then the regularization term. Applying the chain rule we get for $W^{(1)}$ the following:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial W^{(1)}} &= \\frac{\\partial J}{\\partial z^{(2)}} \\cdot \\frac{\\partial z^{(2)}}{\\partial W^{(1)}}\n",
    "\\end{align*}\n",
    "\n",
    "For $\\frac{\\partial J}{\\partial z^{(2)}}$ we apply iteratively the chain rule obtaining:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial z^{(2)}} &= \\frac{\\partial J}{\\partial a^{(2)}} \\odot \\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\\\\n",
    "&= \\left( \\frac{\\partial J}{\\partial z^{(3)}} \\cdot \\frac{\\partial z^{(3)}}{\\partial a^{(2)}} \\right) \\odot \\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where $\\odot$ indicates the element-wise product (Hadamard product) and $\\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\in \\mathbb{R}^{10 \\times 5}$ is the the derivative of the ReLu activation w.r.t. $z^{(2)}$, which is also the heaviside step function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 2kldskla](imgs/fig3_updated.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do already know $\\frac{\\partial J}{\\partial z^{(3)}}$, now the calculation of the other partial derivatives is straightforward:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial z^{(3)}}{\\partial a^{(2)}} &= \\frac{\\partial \\left(W^{(2)} a^{(2)} + b^{(2)} \\right)}{\\partial a^{(2)}} = W^{(2)} \\\\\n",
    "\\frac{\\partial z^{(2)}}{\\partial W^{(1)}} &= \\frac{\\partial \\left(W^{(1)} a^{(1)} + b^{(1)} \\right)}{\\partial W^{(1)}} = a^{(1)}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we finally obtain the loss derivatives w.r.t. $W^{(1)}$ as it follows:\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial W^{(1)}} &= \\frac{\\partial J}{\\partial z^{(2)}} \\cdot \\frac{\\partial z^{(2)}}{\\partial W^{(1)}}\\\\ \n",
    "&= \\left[ \\left( \\frac{\\partial J}{\\partial z^{(3)}} \\cdot \\frac{\\partial z^{(3)}}{\\partial a^{(2)}} \\right) \\odot \\frac{\\partial a^{(2)}}{\\partial z^{(2)}}\\right] \\cdot \\frac{\\partial z^{(2)}}{\\partial W^{(1)}} \\\\\n",
    "&= \\frac{1}{N} \\cdot \\left\\{ \\left[ W^{(2)^T} \\cdot \\left(\\psi(z^{(3)}) - \\Delta \\right) \\right] \\odot \\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\right\\} \\cdot a^{(1)}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now find the gradient of $W^{(1)}$ w.r.t the regularization term, which is:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial \\left[ \\lambda \\left( \\left\\lVert W^{(1)}\\right\\rVert^2_2 + \\left\\lVert W^{(2)}\\right\\rVert^2_2 \\right) \\right]}{\\partial W^{(1)}} = 2 \\lambda W^{(1)}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence we end up having:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial \\tilde{J}}{\\partial W^{(1)}} = \\frac{1}{N} \\cdot \\left\\{ \\left[ W^{(2)^T} \\cdot \\left(\\psi(z^{(3)}) - \\Delta \\right) \\right] \\odot \\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\right\\} \\cdot a^{(1)} +  2 \\lambda W^{(1)}\n",
    "\\end{equation*}\n",
    "\n",
    "Which can be also implemented in vectorized form using numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "\n",
    "We now derive the loss J w.r.t $b^{(1)}$. Applying the chain rule we obtain similarly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial b^{(1)}} &= \\frac{\\partial J}{\\partial z^{(2)}} \\cdot \\frac{\\partial z^{(2)}}{\\partial b^{(1)}}\\\\\n",
    "&= \\left[ \\left( \\frac{\\partial J}{\\partial z^{(3)}} \\cdot \\frac{\\partial z^{(3)}}{\\partial a^{(2)}} \\right) \\odot \\frac{\\partial a^{(2)}}{\\partial z^{(2)}}\\right] \\cdot \\frac{\\partial z^{(2)}}{\\partial b^{(1)}} \\\\\n",
    "&= \\left[ \\left( \\frac{\\partial J}{\\partial z^{(3)}} \\cdot \\frac{\\partial z^{(3)}}{\\partial a^{(2)}} \\right) \\odot \\frac{\\partial a^{(2)}}{\\partial z^{(2)}}\\right] \\cdot  \\mathbb{I} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where $\\mathbb{I}$ in this case denotes a matrix of ones (not the identity matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "\n",
    "For the loss J w.r.t. $b^{(2)}$, similarly we have the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\displaystyle{\\frac{dJ}{db^{(2)}} = \\frac{dJ}{dz^{(3)}} \\cdot \\frac{dz^{(3)}}{db^{(2)}} = \\frac{1}{N}\\left[\\psi(z^{(3)}) - \\Delta \\right] \\cdot \\boldsymbol{\\mathbb{I}}}\n",
    "\\end{equation*}\n",
    "\n",
    "Where $\\mathbb{I}$ in this case denotes a matrix of ones (not the identity matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement the stochastic gradient descent algorithm in two_layernet.py and run the training on the toy data. Your model should be able to obtain loss = 0.02 on the training set and the training curve should look similar to the one shown in Fig. 2.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We report here the code in which we implemented the stochastic gradient descent, shuffling the full training set at the beginning of each epoch and then picking consecutive batches of size **batch_size**. Computing loss and gradients (the latters with backpropagation) using the function loss() , and then updating the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 2kldskla](imgs/fig0_final_mod.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the results below, the training loss history curve of our toy data shows a similar training curve as the one shown in question 3a. Also the final training loss is $< 0.02$ as suggested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 2kldskla](imgs/fig1_2_mod.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[...] Your task is to debug the model training and come up with better hyper-parameters to improve the performance on the validation set. Visualize the training and validation performance curves to help with this analysis. [...] Once you have tuned your hyper-parameters, and get validation accuracy greater than 48% run your best model on the test set once and report the performance. (report, 5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the optimal combination of hyperparameters, we performed an heuristic procedure (grid search). The final grid on which we trained the models is:\n",
    "\n",
    "- hidden_size = [75, 100]\n",
    "- num_iters = [2000, 3000]\n",
    "- batch_size = [200, 300] \n",
    "- learning_rate = [0.001]\n",
    "- reg = [0.01  , 0.0001, 0.001 , 0.1]\n",
    "\n",
    "Which yielded to 32 configurations to be tried. We report here the best 15 results sorted by accuracy on the validation set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure sdajdsiad](imgs/grid_top15_mod.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that all of our first 15 \"best models\" have a greater accuracy than the one proposed in the question 3b ($>48\\%$). In order to help us with the best model choice, we plot the curves of the loss trend and the accuracy history during training of the first four top models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 2kldskla](imgs/plot1_mod.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters:** hidden_size = 75, num_iters = 3000, batch_size = 300, learning_rate= 0.001, reg = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 2kldskla](imgs/plot2_mod.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters:** hidden_size = 100, num_iters = 3000, batch_size = 300, learning_rate= 0.001, reg = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 2kldskla](imgs/plot3_mod.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters:** hidden_size = 75, num_iters = 3000, batch_size = 300, learning_rate= 0.001, reg = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 2kldskla](imgs/plot4_mod.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters:** hidden_size = 100, num_iters = 3000, batch_size = 300, learning_rate= 0.001, reg = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training seems similar for all the models, with the loss which fluctuates along the iterations due to its mini-batch approximations. However we notice that all of the models exhibit mild overfitting tendencies after the 10th epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete the code to implement a multi-layer perceptron network in the class MultiLayerPerceptron in ex2 pytorch.py. This includes instantiating the required layers from torch.nn and writing the code for forward pass. Initially you should write the code for the same two-layer network we have seen before.**\n",
    "\n",
    "The implementation was quite straightforward using nn.Sequential and the torch layers nn.Linear, nn.ReLU, etc.\n",
    "\n",
    "(for further information see the code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing our pytorch code with similar parameters w.r.t to our best implementation in the previous grid-search of Exercise 3b) we got a validation accuracy of 50%. Here we can see the training process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure sdajdsiad](imgs/4b_epochs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that you can train the two layer network to achieve reasonable performance, try increasing the network depth to see if you can improve the performance. Experiment with networks of at least 2, 3, 4, and 5 layers, of your chosen configuration. Report the training and validation accuracies for these models and discuss your observations. Run the evaluation on the test set with your best model and report the test accuracy.**\n",
    "\n",
    "These are the results for the loss train and validation accuracy, obtained by trial and error with several runs, using different numbers of hidden layers (up to 5) as well as different combinations of hyperparameters (batch_size, learning_rate, lr_decay, reg). We observe that the accuracy value drastically decreases by increasing the number of hidden layers, highlighting a saturation of the network, which is thus unable to behave as an effective classifier (getting stuck around 8% accuracy).\n",
    "\n",
    "NB: note that we are not including the output layer among the hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure sdajdsiad](imgs/SidGrearch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
