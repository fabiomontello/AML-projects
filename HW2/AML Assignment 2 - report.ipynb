{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> Fabio Montello (1834411), Francesco Russo (1449025), Michele Cernigliaro (1869097)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this report we proceed to answer as requested the questions 2 (a, b, c), 3 (a,b) and 4 (a,b,c). For each point we are going to write a brief description using figures and formulas whenever needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verify that the loss function defined in Eq. (5) has the gradient w.r.t. $z^{(3)}$ as below:**\n",
    "\n",
    "$$\n",
    "\\frac { \\partial J } { \\partial z ^ { ( 3 ) } } ( \\{ x _ { i } , y _ { i } \\} _ { i = 1 } ^ { N } ) = \\frac { 1 } { N } ( \\psi ( z ^ { ( 3 ) } ) - \\Delta )\n",
    "$$\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verify that the partial derivative of the loss w.r.t. $W^{(2)}$ is:**\n",
    "\n",
    "\\begin{align}\n",
    "\\frac { \\partial J } { \\partial W ^ { ( 3 ) } } ( \\{ x _ { i } , y _ { i } \\} _ { i = 1 } ^ { N } ) &= \\frac{\\partial J}{ \\partial z^{(3)}} \\cdot \\frac{\\partial z^{(3)}}{\\partial W^{(2)}} \\\\\n",
    "&= \\frac { 1 } { N } ( \\psi ( z ^ { ( 3 ) } ) - \\Delta ) a ^ { ( 2 ) }\n",
    "\\end{align}\n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Similarly, verify that the regularized loss in Eq. (6) has the derivatives**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\tilde{J}}{\\partial W^{(2)}} = \\frac { 1 } { N } ( \\psi ( z ^ { ( 3 ) } ) - \\Delta ) a ^ { ( 2 ) } + 2 \\lambda W^{(2)}\n",
    "$$\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2c\n",
    "\n",
    "**We can repeatedly apply chain rule as discussed above to obtain the derivatives of the loss with respect to all the parameters of the model $\\theta = 􏰀\\left(W^{(1)},  􏰀b^{(1)},  􏰀W^{(2)},  􏰀b^{(2)}\\right)$􏰁. Derive the expressions for the derivatives of the regularized loss in Eq. (6) w.r.t. $W^{(1)},  􏰀b^{(1)},  􏰀b^{(2)}$ now.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement the stochastic gradient descent algorithm in two_layernet.py and run the training on the toy data. Your model should be able to obtain loss = 0.02 on the training set and the training curve should look similar to the one shown in Fig. 2.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[...] Your task is to debug the model training and come up with better hyper-parameters to improve the performance on the validation set. Visualize the training and validation performance curves to help with this analysis. [...] Once you have tuned your hyper-parameters, and get validation accuracy greater than 48% run your best model on the test set once and report the performance. (report, 5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete the code to implement a multi-layer perceptron network in the class MultiLayerPerceptron in ex2 pytorch.py. This includes instantiating the required layers from torch.nn and writing the code for forward pass. Initially you should write the code for the same two-layer network we have seen before.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete the code to train the network. Make use of the loss function torch.nn.CrossEntropyLoss to compute the loss and loss.backward() to compute the gradients. Once gradients are computed, optimizer.step() can be invoked to update the model. Your should be able to achieve similar performance (> 48% accuracy on the validation set) as in Q3. Report the final validation accuracy you achieve with a two-layer network. (3 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that you can train the two layer network to achieve reasonable performance, try increasing the network depth to see if you can improve the performance. Experiment with networks of at least 2, 3, 4, and 5 layers, of your chosen configuration. Report the training and validation accuracies for these models and discuss your observations. Run the evaluation on the test set with your best model and report the test accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
