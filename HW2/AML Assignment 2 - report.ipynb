{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> Fabio Montello (1834411), Francesco Russo (1449025), Michele Cernigliaro (1869097)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this report we proceed to answer as requested the questions 2 (a, b, c), 3 (a,b) and 4 (a,b,c). For each point we are going to write a brief description using figures and formulas whenever needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verify that the loss function defined in Eq. (5) has the gradient w.r.t. $z^{(3)}$ as below:**\n",
    "\n",
    "$$\n",
    "\\frac { \\partial J } { \\partial z ^ { ( 3 ) } } ( \\{ x _ { i } , y _ { i } \\} _ { i = 1 } ^ { N } ) = \\frac { 1 } { N } ( \\psi ( z ^ { ( 3 ) } ) - \\Delta )\n",
    "$$\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "$\\displaystyle{J = \\frac{1}{N} \\left[ \\sum_{i=1}^N -log \\space \\psi(z_{yi}) \\right] = \\frac{1}{N} \\left[ \\sum_{i=1}^N -log \\displaystyle{ \\frac{e^{z_{y_i}}}{\\sum_j e^{z_j}} } \\right] = \\frac{1}{N} \\left[ \\sum_{i=1, j = k_i}^N -log \\displaystyle{ \\frac{e^{z_{i,j}}}{\\sum_j e^{z_j}} } \\right]}$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where we use the notation $z = z^{(3)}$ and the softmax activation function is \n",
    "\n",
    "<center>\n",
    "$\\displaystyle{ \\psi(z_{yi}) = \\frac{e^{z_{y_i}}}{\\sum_j e^{z_j}} = \\frac{e^{z_{i k_i}}}{\\sum_j e^{z_j}} }$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix $\\nabla_J$ of the derivatives of $J$ with respect to $z_{ij}$ for every $i = 1...N$ and for every $j = 1...K$ has the general element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\displaystyle{\n",
    "\\nabla_{J_{i',j'}} = \\frac{\\partial J}{\\partial z_{i'j'}} = \n",
    "\\frac{\\partial}{\\partial z_{i'j'}} \\frac{1}{N} \\left[ \\sum_{i=1}^N -log \\space \\psi(z_{yi}) \\right] = \n",
    "- \\frac{\\partial}{\\partial z_{i'j'}} \\frac{1}{N} \\left[log \\space \\psi(z_{yi'}) \\right] = \n",
    "- \\frac{1}{N} \\frac{1}{\\psi(z_{yi'})} \\frac{\\partial}{\\partial z_{i'j'}} \\psi(z_{yi'}) =\n",
    "}$$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\displaystyle{\n",
    "- \\frac{1}{N} \\frac{1}{\\psi(z_{yi'})} \\frac{e^{z_{i'j'}} \\cdot \\delta(j', k_{i'}) \\sum_j e^{z_{i'j}} - e^{z_{i'k_i'}}e^{z_{i'j'}}}{(\\sum_j e^{z_{i'j}})^2} = \n",
    "}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$$\\displaystyle{\n",
    "- \\frac{1}{N} \\frac{\\sum_j e^{z_{i'j}}}{e^{z_{i'k_i'}}} \\frac{e^{z_{i'j'}} \\cdot \\delta(j', k_{i'}) \\sum_j e^{z_{i'j}} - e^{z_{i'k_i'}}e^{z_{i'j'}}}{(\\sum_j e^{z_{i'j}})^2} =\n",
    "}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\displaystyle{\n",
    "\\frac{1}{N} \\left[\\frac{e^{z_{i'j'}}}{\\sum_j e^{z_{i'j}}} - \\delta(j', k_i') \\right]\n",
    "}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\delta(j', k_i')$ is the Kronecker Delta function\n",
    "\n",
    "In matrix form this is exactly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\displaystyle{\n",
    "\\nabla_J = \\frac{1}{N} \\left[\\psi(z) -  \\Delta_{j, ki} \\right]\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verify that the partial derivative of the loss w.r.t. $W^{(2)}$ is:**\n",
    "\n",
    "\\begin{align}\n",
    "\\frac { \\partial J } { \\partial W ^ { ( 2 ) } } ( \\{ x _ { i } , y _ { i } \\} _ { i = 1 } ^ { N } ) &= \\frac{\\partial J}{ \\partial z^{(3)}} \\cdot \\frac{\\partial z^{(3)}}{\\partial W^{(2)}} \\\\\n",
    "&= \\frac { 1 } { N } ( \\psi ( z ^ { ( 3 ) } ) - \\Delta ) a ^ { ( 2 ) }\n",
    "\\end{align}\n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen before, we compute the chain rule as follow:\n",
    "\n",
    "$$\\frac { \\partial J } { \\partial W ^ { ( 2 ) } } ( \\{ x _ { i } , y _ { i } \\} _ { i = 1 } ^ { N } ) = \\frac{\\partial J}{ \\partial z^{(3)}} \\cdot \\frac{\\partial z^{(3)}}{\\partial W^{(2)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already computed:\n",
    "$$ \\frac { 1 } { N } ( \\psi ( z ^ { ( 3 ) } ) - \\Delta )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that $$\\frac{\\partial z^{(3)}}{\\partial W^{(2)}} = a ^ { ( 2 ) }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so we have that:\n",
    "$$\\frac { \\partial J } { \\partial W ^ { ( 2 ) } } ( \\{ x _ { i } , y _ { i } \\} _ { i = 1 } ^ { N } ) = \\frac { 1 } { N } ( \\psi ( z ^ { ( 3 ) } ) - \\Delta ) a ^ { ( 2 ) }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Similarly, verify that the regularized loss in Eq. (6) has the derivatives**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\tilde{J}}{\\partial W^{(2)}} = \\frac { 1 } { N } ( \\psi ( z ^ { ( 3 ) } ) - \\Delta ) a ^ { ( 2 ) } + 2 \\lambda W^{(2)}\n",
    "$$\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now find the partial derivative of the loss w.r.t the regularization term, which is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\left[ \\lambda \\left( \\left\\lVert W^{(1)}\\right\\rVert^2_2 + \\left\\lVert W^{(2)}\\right\\rVert^2_2 \\right) \\right]}{\\partial W^{(2)}} = 2 \\lambda W^{(2)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we obtain:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\displaystyle{\\frac{dJ}{dW^{(2)}} = \\frac{dJ}{dz^{(3)}} \\cdot \\frac{dz^{(3)}}{dW^{(2)}} = \\frac{1}{N}\\left[\\psi(z^{(3)}) - \\Delta \\right] \\cdot a^{(2)} + 2 \\lambda W^{(2)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2c\n",
    "\n",
    "**We can repeatedly apply chain rule as discussed above to obtain the derivatives of the loss with respect to all the parameters of the model $\\theta =\\left(W^{(1)}, b^{(1)}, W^{(2)}, b^{(2)}\\right)$. Derive the expressions for the derivatives of the regularized loss in Eq. (6) w.r.t. $W^{(1)}, b^{(1)}, b^{(2)}$ now.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start deriving the loss J w.r.t $W^{(1)}$, by applying the chain rule. We'll had then the regularization term. Applying the chain rule we get for $W^{(1)}$ the following:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial W^{(1)}} &= \\frac{\\partial J}{\\partial z^{(2)}} \\cdot \\frac{\\partial z^{(2)}}{\\partial W^{(1)}}\\\\\n",
    "\\end{align}\n",
    "\n",
    "For $\\frac{\\partial J}{\\partial z^{(2)}}$ we apply iteratively the chain rule obtaining:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial z^{(2)}} &= \\frac{\\partial J}{\\partial a^{(2)}} \\odot \\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\\\\n",
    "&= \\left( \\frac{\\partial J}{\\partial z^{(3)}} \\cdot \\frac{\\partial z^{(3)}}{\\partial a^{(2)}} \\right) \\odot \\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\\\\n",
    "\\end{align}\n",
    "\n",
    "Where $\\odot$ indicates the element-wise product (Hadamard product) and $\\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\in \\mathbb{R}^{10 \\times 5}$ is the the derivative of the ReLu activation w.r.t. $z^{(2)}$, which is also the heaviside step function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 2kldskla](imgs/fig3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do already know $\\frac{\\partial J}{\\partial z^{(3)}}$, now the calculation of the other partial derivatives is straightforward:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial z^{(3)}}{\\partial a^{(2)}} &= \\frac{\\partial \\left(W^{(2)} a^{(2)} + b^{(2)} \\right)}{\\partial a^{(2)}} = W^{(2)} \\\\\n",
    "\\frac{\\partial z^{(2)}}{\\partial W^{(1)}} &= \\frac{\\partial \\left(W^{(1)} a^{(1)} + b^{(1)} \\right)}{\\partial W^{(1)}} = a^{(1)}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we finally obtain the loss derivatives w.r.t. $W^{(1)}$ as it follows:\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial W^{(1)}} &= \\frac{\\partial J}{\\partial z^{(2)}} \\cdot \\frac{\\partial z^{(2)}}{\\partial W^{(1)}}\\\\ \n",
    "&= \\left[ \\left( \\frac{\\partial J}{\\partial z^{(3)}} \\cdot \\frac{\\partial z^{(3)}}{\\partial a^{(2)}} \\right) \\odot \\frac{\\partial a^{(2)}}{\\partial z^{(2)}}\\right] \\cdot \\frac{\\partial z^{(2)}}{\\partial W^{(1)}} \\\\\n",
    "&= \\frac{1}{N} \\cdot \\left\\{ \\left[ W^{(2)^T} \\cdot \\left(\\psi(z^{(3)}) - \\Delta \\right) \\right] \\odot \\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\right\\} \\cdot a^{(1)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now find the gradient of $W^{(1)}$ w.r.t the regularization term, which is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\left[ \\lambda \\left( \\left\\lVert W^{(1)}\\right\\rVert^2_2 + \\left\\lVert W^{(2)}\\right\\rVert^2_2 \\right) \\right]}{\\partial W^{(1)}} = 2 \\lambda W^{(1)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence we end up having:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\tilde{J}}{\\partial W^{(1)}} = \\frac{1}{N} \\cdot \\left\\{ \\left[ W^{(2)^T} \\cdot \\left(\\psi(z^{(3)}) - \\Delta \\right) \\right] \\odot \\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\right\\} \\cdot a^{(1)} +  2 \\lambda W^{(1)}\n",
    "$$\n",
    "\n",
    "Which can be also implemented in vectorized form using numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "\n",
    "We now derive the loss J w.r.t $b^{(1)}$. Applying the chain rule we obtain similarly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial b^{(1)}} &= \\frac{\\partial J}{\\partial z^{(2)}} \\cdot \\frac{\\partial z^{(2)}}{\\partial b^{(1)}}\\\\\n",
    "&= \\left[ \\left( \\frac{\\partial J}{\\partial z^{(3)}} \\cdot \\frac{\\partial z^{(3)}}{\\partial a^{(2)}} \\right) \\odot \\frac{\\partial a^{(2)}}{\\partial z^{(2)}}\\right] \\cdot \\frac{\\partial z^{(2)}}{\\partial W^{(1)}} \\\\\n",
    "&= \\left[ \\left( \\frac{\\partial J}{\\partial z^{(3)}} \\cdot \\frac{\\partial z^{(3)}}{\\partial a^{(2)}} \\right) \\odot \\frac{\\partial a^{(2)}}{\\partial z^{(2)}}\\right] \\cdot  \\mathbb{1} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "\n",
    "For the loss J w.r.t. $b^{(2)}$, similarly we have the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\displaystyle{\\frac{dJ}{db^{(2)}} = \\frac{dJ}{dz^{(3)}} \\cdot \\frac{dz^{(3)}}{db^{(2)}} = \\frac{1}{N}\\left[\\psi(z^{(3)}) - \\Delta \\right] \\cdot \\mathbb{1}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement the stochastic gradient descent algorithm in two_layernet.py and run the training on the toy data. Your model should be able to obtain loss = 0.02 on the training set and the training curve should look similar to the one shown in Fig. 2.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We report here the code in which we implemented the stochastic gradient descent, sampling (pseudo-)randomly batches of size **batch_size** from the training set. Computing loss and gradients (the latters with backpropagation) using the function _loss()_, and then updating the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 2kldskla](imgs/fig0_final.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the results below, the training loss history curve for how toy data shows a similar training curve as the one shown in question 3a. Also the final training loss is $\\sim 0.02$ as suggested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 2kldskla](imgs/fig1_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[...] Your task is to debug the model training and come up with better hyper-parameters to improve the performance on the validation set. Visualize the training and validation performance curves to help with this analysis. [...] Once you have tuned your hyper-parameters, and get validation accuracy greater than 48% run your best model on the test set once and report the performance. (report, 5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the optimal combination of hyperparameters, we performed an heuristic procedure (grid search). We report here the best 15 results sorted by accuracy on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>num_iters</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>reg</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>3000</td>\n",
       "      <td>300</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>1.147523</td>\n",
       "      <td>1.331869</td>\n",
       "      <td>0.604347</td>\n",
       "      <td>0.539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>3000</td>\n",
       "      <td>300</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.134403</td>\n",
       "      <td>1.340498</td>\n",
       "      <td>0.609735</td>\n",
       "      <td>0.536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>3000</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1.153017</td>\n",
       "      <td>1.343609</td>\n",
       "      <td>0.602490</td>\n",
       "      <td>0.533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>3000</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>1.183392</td>\n",
       "      <td>1.370682</td>\n",
       "      <td>0.588980</td>\n",
       "      <td>0.525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75</td>\n",
       "      <td>3000</td>\n",
       "      <td>300</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.179377</td>\n",
       "      <td>1.370900</td>\n",
       "      <td>0.596204</td>\n",
       "      <td>0.525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>75</td>\n",
       "      <td>3000</td>\n",
       "      <td>300</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>1.176405</td>\n",
       "      <td>1.361328</td>\n",
       "      <td>0.594592</td>\n",
       "      <td>0.522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100</td>\n",
       "      <td>3000</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.145084</td>\n",
       "      <td>1.362785</td>\n",
       "      <td>0.603265</td>\n",
       "      <td>0.522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100</td>\n",
       "      <td>2000</td>\n",
       "      <td>300</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.245536</td>\n",
       "      <td>1.369817</td>\n",
       "      <td>0.565898</td>\n",
       "      <td>0.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>75</td>\n",
       "      <td>2000</td>\n",
       "      <td>300</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1.282174</td>\n",
       "      <td>1.393147</td>\n",
       "      <td>0.554673</td>\n",
       "      <td>0.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100</td>\n",
       "      <td>2000</td>\n",
       "      <td>300</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>1.266654</td>\n",
       "      <td>1.384809</td>\n",
       "      <td>0.561796</td>\n",
       "      <td>0.519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100</td>\n",
       "      <td>2000</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>1.256129</td>\n",
       "      <td>1.369715</td>\n",
       "      <td>0.559224</td>\n",
       "      <td>0.519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>75</td>\n",
       "      <td>3000</td>\n",
       "      <td>300</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1.177504</td>\n",
       "      <td>1.382315</td>\n",
       "      <td>0.591612</td>\n",
       "      <td>0.516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>75</td>\n",
       "      <td>2000</td>\n",
       "      <td>300</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>1.275136</td>\n",
       "      <td>1.388821</td>\n",
       "      <td>0.557367</td>\n",
       "      <td>0.513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>75</td>\n",
       "      <td>3000</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>1.213762</td>\n",
       "      <td>1.381529</td>\n",
       "      <td>0.580714</td>\n",
       "      <td>0.512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>100</td>\n",
       "      <td>2000</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.257539</td>\n",
       "      <td>1.394553</td>\n",
       "      <td>0.563449</td>\n",
       "      <td>0.512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hidden_size  num_iters  batch_size  learning_rate     reg  train_loss  \\\n",
       "0           100       3000         300          0.001  0.0100    1.147523   \n",
       "1           100       3000         300          0.001  0.0001    1.134403   \n",
       "2           100       3000         200          0.001  0.0010    1.153017   \n",
       "3           100       3000         200          0.001  0.1000    1.183392   \n",
       "4            75       3000         300          0.001  0.0001    1.179377   \n",
       "5            75       3000         300          0.001  0.0100    1.176405   \n",
       "6           100       3000         200          0.001  0.0001    1.145084   \n",
       "7           100       2000         300          0.001  0.0001    1.245536   \n",
       "8            75       2000         300          0.001  0.0010    1.282174   \n",
       "9           100       2000         300          0.001  0.1000    1.266654   \n",
       "10          100       2000         200          0.001  0.0100    1.256129   \n",
       "11           75       3000         300          0.001  0.0010    1.177504   \n",
       "12           75       2000         300          0.001  0.1000    1.275136   \n",
       "13           75       3000         200          0.001  0.1000    1.213762   \n",
       "14          100       2000         200          0.001  0.0001    1.257539   \n",
       "\n",
       "    val_loss  train_acc  val_acc  \n",
       "0   1.331869   0.604347    0.539  \n",
       "1   1.340498   0.609735    0.536  \n",
       "2   1.343609   0.602490    0.533  \n",
       "3   1.370682   0.588980    0.525  \n",
       "4   1.370900   0.596204    0.525  \n",
       "5   1.361328   0.594592    0.522  \n",
       "6   1.362785   0.603265    0.522  \n",
       "7   1.369817   0.565898    0.521  \n",
       "8   1.393147   0.554673    0.521  \n",
       "9   1.384809   0.561796    0.519  \n",
       "10  1.369715   0.559224    0.519  \n",
       "11  1.382315   0.591612    0.516  \n",
       "12  1.388821   0.557367    0.513  \n",
       "13  1.381529   0.580714    0.512  \n",
       "14  1.394553   0.563449    0.512  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('results_twolayernet.csv')\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to help us with the best model choice, we plot the curves of the loss trend during training of the four bests models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 2kldskla](imgs/plot1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters:** hidden_size = 100, num_iters = 3000, batch_size = 300, learning_rate= 0.001, reg = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 2kldskla](imgs/plot2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters:** hidden_size = 100, num_iters = 3000, batch_size = 300, learning_rate= 0.001, reg = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 2kldskla](imgs/plot3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters:** hidden_size = 100, num_iters = 3000, batch_size = 200, learning_rate= 0.001, reg = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 2kldskla](imgs/plot4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters:** hidden_size = 100, num_iters = 3000, batch_size = 200, learning_rate= 0.001, reg = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commenti gavemo???????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete the code to implement a multi-layer perceptron network in the class MultiLayerPerceptron in ex2 pytorch.py. This includes instantiating the required layers from torch.nn and writing the code for forward pass. Initially you should write the code for the same two-layer network we have seen before.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that you can train the two layer network to achieve reasonable performance, try increasing the network depth to see if you can improve the performance. Experiment with networks of at least 2, 3, 4, and 5 layers, of your chosen configuration. Report the training and validation accuracies for these models and discuss your observations. Run the evaluation on the test set with your best model and report the test accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
